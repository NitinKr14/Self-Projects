# -*- coding: utf-8 -*-
"""Ananya_Singha_18032_assing1_ipynb.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SeTum7QFj6JXiZzdNAgRV49zPTcgmuVj
"""

import csv,sys
from sklearn.ensemble import RandomForestClassifier
from nltk.corpus import stopwords
import re
from bs4 import BeautifulSoup
import logging
import pandas as pd
import numpy as np
from numpy import random
import gensim
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.model_selection import GridSearchCV 
from sklearn.pipeline import Pipeline
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedKFold, train_test_split 
from sklearn.metrics import confusion_matrix,accuracy_score, precision_score, recall_score, f1_score
from nltk.stem import PorterStemmer
from nltk.stem import WordNetLemmatizer
import nltk
nltk.download('all')

f_train_data = open('/content/SEN_training_data.txt','r')
#print(f)
reader_train_data = list(csv.reader(f_train_data,delimiter='\t'))

data_train=[];
for item in reader_train_data:
    #labels.append(item[0])    
    data_train.append(item[0])

f_train_label = open('/content/SEN_training_data_labels.txt','r')
#print(f)
reader_train_label = list(csv.reader(f_train_label,delimiter='\t'))

labels_train=[];
for item in reader_train_label:
    #labels.append(item[0])    
    labels_train.append(item[0])

f_test_data = open('/content/SEN_test_data.txt','r')
#print(f)
reader_test_data = list(csv.reader(f_test_data,delimiter='\t'))

data_test=[];
for item in reader_test_data:
    #labels.append(item[0])    
    data_test.append(item[0])  

analyzer = CountVectorizer().build_analyzer()

def stemmed_words(doc):
  return (PorterStemmer().stem(w) for w in analyzer(doc))

def lemmatized_words(doc):
  return (WordNetLemmatizer().lemmatize(w) for w in analyzer(doc))

print(analyzer(data_train[0]))
t= lemmatized_words(data_train[0])
print(t[0])
print(data_train[0])

REPLACE_BY_SPACE_RE = re.compile('[/(){}\[\]\|@,;]')
BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')
STOPWORDS = set(stopwords.words('english'))

def clean_text(text, type):
    """
        text: a string
        
        return: modified initial string
    """
    # text = BeautifulSoup(text, "lxml").text # HTML decoding
    # text = text.lower() # lowercase text
    # text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text
    # text = BAD_SYMBOLS_RE.sub('', text) # delete symbols which are in BAD_SYMBOLS_RE from text
    #text = ' '.join(word for word in text.split() if word not in STOPWORDS) # delete stopwors from text
    #text = ' '.join(word for word in analyzer(text) if word not in STOPWORDS) # delete stopwors from text
    #text = ' '.join(WordNetLemmatizer().lemmatize(w) for w in analyzer(text))
    if type == 1:
      text = ' '.join(word for word in text.split() if word not in STOPWORDS) # delete stopwors from text
    elif type == 2:
      #print('only_stop')
      text = ' '.join(word for word in analyzer(text) if word not in STOPWORDS)
    
    elif type == 3:
      text = ' '.join(PorterStemmer().stem(w) for w in analyzer(text))
    elif type == 4:
      text = ' '.join(WordNetLemmatizer().lemmatize(w) for w in analyzer(text))
    elif type == 5:
      text = ' '.join(word for word in analyzer(text) if word not in STOPWORDS)
      text = ' '.join(PorterStemmer().stem(w) for w in analyzer(text))
    elif type == 6:
      text = ' '.join(word for word in analyzer(text) if word not in STOPWORDS)
      text = ' '.join(WordNetLemmatizer().lemmatize(w) for w in analyzer(text))
    return text

df_train = pd.DataFrame(data_train)
# print('words_per before',df_train[0].apply(lambda x: len(x.split(' '))).sum())
# df_train[0] = df_train[0].apply(clean_text)
# print('words_per after',df_train[0].apply(lambda x: len(x.split(' '))).sum())
#df_train[0]

# df_train = pd.DataFrame(data_train)
# print('words_per before',df_train[0].apply(lambda x: len(x.split(' '))).sum())
# df_train[0] = df_train[0].apply(clean_text)
# print('words_per after',df_train[0].apply(lambda x: len(x.split(' '))).sum())
# #df_train[0]

# df_test = pd.DataFrame(data_test)
# print('words_per before',df_test[0].apply(lambda x: len(x.split(' '))).sum())
# df_test[0] = df_test[0].apply(clean_text)
# print('words_per after',df_test[0].apply(lambda x: len(x.split(' '))).sum())
# df_test[0]

data_test
#labels_train

import pandas as pd
new_row = {'model':['demo'],'precision': [0],'recall':[0], 'micro_f1': [0],'macro_f1':[0] }
#new_row = {'model':['demo'],'preprocessing_used':['demo'],'precision': [0],'recall':[0], 'micro_f1': [0],'macro_f1':[0] }
df0= pd.DataFrame(new_row) 
df0

def evaluation(trn_data,trn_cat,tst_cat,tst_data,predicted,model, df0):
    print('\n Total documents in the training set: '+str(len(trn_data))+'\n')    
    print('\n Total documents in the test set: '+str(len(tst_data))+'\n')
    print ('\n Confusion Matrix \n')  
    print (confusion_matrix(tst_cat, predicted))  

    pr=precision_score(tst_cat, predicted, average='micro') 
    print ('\n Precision:'+str(pr)) 

    rl=recall_score(tst_cat, predicted, average='micro') 
    print ('\n Recall:'+str(rl))

    fm1=f1_score(tst_cat, predicted, average='macro') 
    print ('\n Macro Averaged F1-Score :'+str(fm1))

    fm2=f1_score(tst_cat, predicted, average='micro') 
    print ('\n Mircro Averaged F1-Score:'+str(fm2))
    #new_row = {'model':model,'preprocessing_used':analyser,'precision': pr,'recall':rl, 'micro_f1': fm1,'macro_f1':fm2 }
    new_row = {'model':model,'precision': pr,'recall':rl, 'micro_f1': fm1,'macro_f1':fm2 }
    df0= df0.append(new_row,ignore_index=True) 
    print(df0)
    return df0

"""#to compare best preprocessing technique

# word to vec

1
"""

!wget -c "https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz"

#%%time
from gensim.models import Word2Vec

wv = gensim.models.KeyedVectors.load_word2vec_format("GoogleNews-vectors-negative300.bin.gz", binary=True)
wv.init_sims(replace=True)


def word_averaging(wv, words):
    all_words, mean = set(), []
    
    for word in words:
        if isinstance(word, np.ndarray):
            mean.append(word)
        elif word in wv.vocab:
            mean.append(wv.syn0norm[wv.vocab[word].index])
            all_words.add(wv.vocab[word].index)

    if not mean:
        logging.warning("cannot compute similarity with no input %s", words)
        # FIXME: remove these examples in pre-processing
        return np.zeros(wv.vector_size,)

    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)
    return mean

def  word_averaging_list(wv, text_list):
    return np.vstack([word_averaging(wv, post) for post in text_list ])


def w2v_tokenize_text(text):
    tokens = []
    for sent in nltk.sent_tokenize(text, language='english'):
        for word in nltk.word_tokenize(sent, language='english'):
            if len(word) < 2:
                continue
            tokens.append(word)
    return tokens

"""#no pre"""

df_train = pd.DataFrame(data_train)
print('words_per before',df_train[0].apply(lambda x: len(x.split(' '))).sum())
df_test = pd.DataFrame(data_test)
df_train[0] = df_train[0].apply(clean_text, args= (1,))
print('words_per after',df_train[0].apply(lambda x: len(x.split(' '))).sum())

train_x, test_x, train_y, test_y = train_test_split(df_train, labels_train, test_size=0.2, random_state = 42)

test_tokenized = test_x.apply(lambda r: w2v_tokenize_text(r[0]), axis=1).values
train_tokenized = train_x.apply(lambda r: w2v_tokenize_text(r[0]), axis=1).values


X_train_word_average = word_averaging_list(wv,train_tokenized)
X_test_word_average = word_averaging_list(wv,test_tokenized)

clf=LogisticRegression(class_weight='balanced', solver='newton-cg')
clf.fit(X_train_word_average,train_y)
predicted = clf.predict(X_test_word_average)
predicted =list(predicted)
#evaluation(train_x,train_y,test_y,test_x,predicted,'no_preprocessing',df)
df0 = evaluation(X_train_word_average,train_y,test_y,X_test_word_average,predicted,'Word2vec','no_preprocessing',df0)

"""# only_stop"""

df_train = pd.DataFrame(data_train)
print('words_per before',df_train[0].apply(lambda x: len(x.split(' '))).sum())

df_train[0] = df_train[0].apply(clean_text, args= (2,))
print('words_per after',df_train[0].apply(lambda x: len(x.split(' '))).sum())

train_x, test_x, train_y, test_y = train_test_split(df_train, labels_train, test_size=0.2, random_state = 42)

test_tokenized = test_x.apply(lambda r: w2v_tokenize_text(r[0]), axis=1).values
train_tokenized = train_x.apply(lambda r: w2v_tokenize_text(r[0]), axis=1).values


X_train_word_average = word_averaging_list(wv,train_tokenized)
X_test_word_average = word_averaging_list(wv,test_tokenized)

clf=LogisticRegression(class_weight='balanced', solver='newton-cg')
clf.fit(X_train_word_average,train_y)
predicted = clf.predict(X_test_word_average)
predicted =list(predicted)
#evaluation(train_x,train_y,test_y,test_x,predicted,'no_preprocessing',df)
df0 = evaluation(X_train_word_average,train_y,test_y,X_test_word_average,predicted,'Word2vec','only_stopwords',df0)

"""# only stem"""

df_train = pd.DataFrame(data_train)
print('words_per before',df_train[0].apply(lambda x: len(x.split(' '))).sum())

df_train[0] = df_train[0].apply(clean_text,args= (3,))
print('words_per after',df_train[0].apply(lambda x: len(x.split(' '))).sum())

train_x, test_x, train_y, test_y = train_test_split(df_train, labels_train, test_size=0.2, random_state = 42)

test_tokenized = test_x.apply(lambda r: w2v_tokenize_text(r[0]), axis=1).values
train_tokenized = train_x.apply(lambda r: w2v_tokenize_text(r[0]), axis=1).values


X_train_word_average = word_averaging_list(wv,train_tokenized)
X_test_word_average = word_averaging_list(wv,test_tokenized)

clf=LogisticRegression(class_weight='balanced', solver='newton-cg')
clf.fit(X_train_word_average,train_y)
predicted = clf.predict(X_test_word_average)
predicted =list(predicted)
#evaluation(train_x,train_y,test_y,test_x,predicted,'no_preprocessing',df)
df0 = evaluation(X_train_word_average,train_y,test_y,X_test_word_average,predicted,'Word2vec','only_stemmed_words',df0)

"""#only lem"""

df_train = pd.DataFrame(data_train)
print('words_per before',df_train[0].apply(lambda x: len(x.split(' '))).sum())

df_train[0] = df_train[0].apply(clean_text, args= (4,))
print('words_per after',df_train[0].apply(lambda x: len(x.split(' '))).sum())

train_x, test_x, train_y, test_y = train_test_split(df_train, labels_train, test_size=0.2, random_state = 42)

test_tokenized = test_x.apply(lambda r: w2v_tokenize_text(r[0]), axis=1).values
train_tokenized = train_x.apply(lambda r: w2v_tokenize_text(r[0]), axis=1).values


X_train_word_average = word_averaging_list(wv,train_tokenized)
X_test_word_average = word_averaging_list(wv,test_tokenized)

clf=LogisticRegression(class_weight='balanced', solver='newton-cg')
clf.fit(X_train_word_average,train_y)
predicted = clf.predict(X_test_word_average)
predicted =list(predicted)
#evaluation(train_x,train_y,test_y,test_x,predicted,'no_preprocessing',df)
df0 = evaluation(X_train_word_average,train_y,test_y,X_test_word_average,predicted,'Word2vec','only_lemmatized_words',df0)

"""#bstem"""

df_train = pd.DataFrame(data_train)
print('words_per before',df_train[0].apply(lambda x: len(x.split(' '))).sum())

df_train[0] = df_train[0].apply(clean_text,args= (5,))
print('words_per after',df_train[0].apply(lambda x: len(x.split(' '))).sum())

train_x, test_x, train_y, test_y = train_test_split(df_train, labels_train, test_size=0.2, random_state = 42)

test_tokenized = test_x.apply(lambda r: w2v_tokenize_text(r[0]), axis=1).values
train_tokenized = train_x.apply(lambda r: w2v_tokenize_text(r[0]), axis=1).values


X_train_word_average = word_averaging_list(wv,train_tokenized)
X_test_word_average = word_averaging_list(wv,test_tokenized)

clf=LogisticRegression(class_weight='balanced', solver='newton-cg')
clf.fit(X_train_word_average,train_y)
predicted = clf.predict(X_test_word_average)
predicted =list(predicted)
#evaluation(train_x,train_y,test_y,test_x,predicted,'no_preprocessing',df)
df0 = evaluation(X_train_word_average,train_y,test_y,X_test_word_average,predicted,'Word2vec','stopword + stemmed_words',df0)

"""#blem"""

df_train = pd.DataFrame(data_train)
print('words_per before',df_train[0].apply(lambda x: len(x.split(' '))).sum())

df_train[0] = df_train[0].apply(clean_text, args= (6,))
print('words_per after',df_train[0].apply(lambda x: len(x.split(' '))).sum())

train_x, test_x, train_y, test_y = train_test_split(df_train, labels_train, test_size=0.2, random_state = 42)

test_tokenized = test_x.apply(lambda r: w2v_tokenize_text(r[0]), axis=1).values
train_tokenized = train_x.apply(lambda r: w2v_tokenize_text(r[0]), axis=1).values


X_train_word_average = word_averaging_list(wv,train_tokenized)
X_test_word_average = word_averaging_list(wv,test_tokenized)

clf=LogisticRegression(class_weight='balanced', solver='newton-cg')
clf.fit(X_train_word_average,train_y)
predicted = clf.predict(X_test_word_average)
predicted =list(predicted)
#evaluation(train_x,train_y,test_y,test_x,predicted,'no_preprocessing',df)
df0 = evaluation(X_train_word_average,train_y,test_y,X_test_word_average,predicted,'Word2vec','stopword +lemmatized_words',df0)

X_train_word_average

"""# Bag of words

# 0. nothing
"""

len(data_train),len(labels_train)

#clf=MultinomialNB(alpha=0,fit_prior=True, class_prior=None)
clf=LogisticRegression(class_weight='balanced', solver='newton-cg')  
vectorizer=TfidfVectorizer(ngram_range=(2,3),token_pattern=r'\b\w+\b')
tfidf = vectorizer.fit_transform(data_train)
tfidf = tfidf.toarray()

# Training and Test Split           
  
train_x, test_x, train_y, test_y = train_test_split(tfidf, labels_train, test_size=0.20, random_state=42,stratify=labels_train)

#Classificaion    
clf.fit(train_x,train_y)
predicted = clf.predict(test_x)
predicted =list(predicted)
#evaluation(train_x,train_y,test_y,test_x,predicted,'no_preprocessing',df)
df0 = evaluation(train_x,train_y,test_y,test_x,predicted,'Bag of words','no_preprocessing',df0)

tfidf.shape

predicted

"""#1. only stop word"""

#clf=MultinomialNB(alpha=0,fit_prior=True, class_prior=None) 
clf=LogisticRegression(class_weight='balanced', solver='newton-cg')  
vectorizer=TfidfVectorizer(stop_words='english',ngram_range=(2,3),token_pattern=r'\b\w+\b')
tfidf = vectorizer.fit_transform(data_train)
tfidf = tfidf.toarray()
# Training and Test Split           
  
train_x, test_x, train_y, test_y = train_test_split(tfidf, labels_train, test_size=0.20, random_state=42,stratify=labels_train)

#Classificaion    
clf.fit(train_x,train_y)
predicted = clf.predict(test_x)
predicted =list(predicted)
df0 =evaluation(train_x,train_y,test_y,test_x,predicted,'Bag of words','only_stopword',df0)

"""#. only stemmed_words"""

#clf=MultinomialNB(alpha=0,fit_prior=True, class_prior=None) 
clf=LogisticRegression(class_weight='balanced', solver='newton-cg')   
vectorizer=TfidfVectorizer(analyzer = stemmed_words,ngram_range=(2,3),token_pattern=r'\b\w+\b')
tfidf = vectorizer.fit_transform(data_train)
tfidf = tfidf.toarray()
# Training and Test Split           
  
train_x, test_x, train_y, test_y = train_test_split(tfidf, labels_train, test_size=0.20, random_state=42,stratify=labels_train)

#Classificaion    
clf.fit(train_x,train_y)
predicted = clf.predict(test_x)
predicted =list(predicted)
df0 = evaluation(train_x,train_y,test_y,test_x,predicted,'Bag of words', 'only_stemmed_words',df0)

"""# only lemmatized_words"""

#clf=MultinomialNB(alpha=0,fit_prior=True, class_prior=None) 
clf=LogisticRegression(class_weight='balanced', solver='newton-cg')  
vectorizer=TfidfVectorizer(analyzer = lemmatized_words,ngram_range=(2,3),token_pattern=r'\b\w+\b')
tfidf = vectorizer.fit_transform(data_train)
tfidf = tfidf.toarray()
# Training and Test Split           
  
train_x, test_x, train_y, test_y = train_test_split(tfidf, labels_train, test_size=0.20, random_state=42,stratify=labels_train)

#Classificaion    
clf.fit(train_x,train_y)
predicted = clf.predict(test_x)
predicted =list(predicted)
df0= evaluation(train_x,train_y,test_y,test_x,predicted,'Bag of words', 'only_lemmatized_words',df0)

"""# stopword + stemmed_words"""

#clf=MultinomialNB(alpha=0,fit_prior=True, class_prior=None)
clf=LogisticRegression(class_weight='balanced', solver='newton-cg')   
vectorizer=TfidfVectorizer(stop_words='english',analyzer = stemmed_words, ngram_range=(2,3),token_pattern=r'\b\w+\b')
tfidf = vectorizer.fit_transform(data_train)
tfidf = tfidf.toarray()
# Training and Test Split           
  
train_x, test_x, train_y, test_y = train_test_split(tfidf, labels_train, test_size=0.20, random_state=42,stratify=labels_train)

#Classificaion    
clf.fit(train_x,train_y)
predicted = clf.predict(test_x)
predicted =list(predicted)
df0 = evaluation(train_x,train_y,test_y,test_x,predicted, 'Bag of words','stopword + stemmed_words', df0)

"""# stopword +lemmatized_words"""

#clf=MultinomialNB(alpha=0,fit_prior=True, class_prior=None)  
clf=LogisticRegression(class_weight='balanced', solver='newton-cg') 
vectorizer=TfidfVectorizer(stop_words='english',analyzer = lemmatized_words,ngram_range=(2,3),token_pattern=r'\b\w+\b')
tfidf = vectorizer.fit_transform(data_train)
tfidf = tfidf.toarray()
# Training and Test Split           
  
train_x, test_x, train_y, test_y = train_test_split(tfidf, labels_train, test_size=0.20, random_state=42,stratify=labels_train)

#Classificaion    
clf.fit(train_x,train_y)
predicted = clf.predict(test_x)
predicted =list(predicted)
df0 =evaluation(train_x,train_y,test_y,test_x,predicted,'Bag of words', 'stopword +lemmatized_words', df0)

dfoo = df0.drop(0)
dfoo = dfoo.drop(13)
dfoo

dfoo.to_csv('/content/preprocessing_result_csv.csv', index= None)

vectorizer = TfidfVectorizer(stop_words='english',ngram_range=(1,1))
X = vectorizer.fit_transform(data_train)
#create dataframe
cv_dataframe=pd.DataFrame(X.toarray(),columns=vectorizer.get_feature_names())
print(cv_dataframe)

X_train_word_average.shape,X_test_word_average.shape

"""#logistic_regression"""

train_x, test_x, train_y, test_y = train_test_split(data_train, labels_train, test_size=0.20, random_state=42,stratify=labels_train)
#Classifier
clf=LogisticRegression(class_weight='balanced') 
clf_parameters = {'clf__solver':('newton-cg','lbfgs','liblinear'),}    
        
#Feature Extraction
pipeline = Pipeline([
('vect', CountVectorizer(token_pattern=r'\b\w+\b')),
('tfidf', TfidfTransformer(use_idf=True,smooth_idf=True)),     
('clf', clf),]) 
    
feature_parameters = {
"vect__stop_words": (None, 'english'),
'vect__analyzer':(stemmed_words, lemmatized_words),
'vect__min_df': (2,3),
'vect__ngram_range': ((1,1),(1, 2),(1,3),(2,3)),  # Unigrams, Bigrams or Trigrams
}

#Classificaion
parameters={**feature_parameters,**clf_parameters} 
grid = GridSearchCV(pipeline,parameters,scoring='f1_micro',cv=10)          
grid.fit(train_x,train_y)     
clf= grid.best_estimator_  
print('********* Best Set of Parameters ********* \n\n')
print(clf)

predicted = clf.predict(test_x)
predicted =list(predicted)
df0 = evaluation(train_x,train_y,test_y,test_x,predicted,'logistic_regression',df0)

results = clf.predict(data_test)
results.shape

len(results), len(data_test)

results

df11 = pd.DataFrame()
df11['labels']= results
df11

df11.to_csv('test_labels.csv')

"""#naive_baies"""

train_x, test_x, train_y, test_y = train_test_split(data_train, labels_train, test_size=0.20, random_state=42,stratify=labels_train)
#Classifier
clf=MultinomialNB(fit_prior=True, class_prior=None)

clf_parameters = {'clf__alpha':(0,1,2),}    
        
#Feature Extraction
pipeline = Pipeline([
('vect', CountVectorizer(token_pattern=r'\b\w+\b')),
('tfidf', TfidfTransformer(use_idf=True,smooth_idf=True)),     
('clf', clf),]) 
    
feature_parameters = {
"vect__stop_words": (None, 'english'),
'vect__analyzer':(stemmed_words, lemmatized_words),
'vect__min_df': (2,3),
'vect__ngram_range': ((1,1),(1, 2),(1,3),(2,3)),  # Unigrams, Bigrams or Trigrams
}

#Classificaion
parameters={**feature_parameters,**clf_parameters} 
grid = GridSearchCV(pipeline,parameters,scoring='f1_micro',cv=10)          
grid.fit(train_x,train_y)     
clf= grid.best_estimator_  
print('********* Best Set of Parameters ********* \n\n')
print(clf)

predicted = clf.predict(test_x)
predicted =list(predicted)
df0 = evaluation(train_x,train_y,test_y,test_x,predicted, 'naive_bayes',df0)

"""#SVM"""

from sklearn.svm import SVC
train_x, test_x, train_y, test_y = train_test_split(data_train, labels_train, test_size=0.20, random_state=42,stratify=labels_train)
#Classifier
clf=SVC()

clf_parameters = {'clf__kernel':('linear', 'poly'),#, 'rbf', 'sigmoid'),
                  'clf__C' : (0,1),#,3,5,10),
                  }    
        
#Feature Extraction
pipeline = Pipeline([
('vect', CountVectorizer(token_pattern=r'\b\w+\b')),
('tfidf', TfidfTransformer(use_idf=True,smooth_idf=True)),     
('clf', clf),]) 
    
feature_parameters = {
#"vect__stop_words": (None, 'english'),
'vect__analyzer':(stemmed_words, lemmatized_words),
'vect__min_df': (2,3),
'vect__ngram_range': ((2,3),(1,1))#,(1, 2),(1,3)),  # Unigrams, Bigrams or Trigrams
}

#Classificaion
parameters={**feature_parameters,**clf_parameters} 
grid = GridSearchCV(pipeline,parameters,scoring='f1_micro',cv=10)          
grid.fit(train_x,train_y)     
clf= grid.best_estimator_  
print('********* Best Set of Parameters ********* \n\n')
print(clf)

predicted = clf.predict(test_x)
predicted =list(predicted)
df0 = evaluation(train_x,train_y,test_y,test_x,predicted,'svm',df0)

"""#random forest"""

from sklearn.svm import SVC
train_x, test_x, train_y, test_y = train_test_split(data_train, labels_train, test_size=0.20, random_state=42,stratify=labels_train)
#Classifier
clf=RandomForestClassifier(random_state=42,n_estimators = 10)

clf_parameters = {'clf__criterion':('gini', 'entropy'),
                  'clf__max_depth' : (65,66,67,68,69,70),
                  'clf__max_features':('auto',0.75,0.8,0.4),
                  }    
        
#Feature Extraction
pipeline = Pipeline([
('vect', CountVectorizer(token_pattern=r'\b\w+\b')),
('tfidf', TfidfTransformer(use_idf=True,smooth_idf=True)),     
('clf', clf),]) 
    
feature_parameters = {
#"vect__stop_words": (None, 'english'),
'vect__analyzer':(stemmed_words, lemmatized_words),
'vect__min_df': (2,3),
'vect__ngram_range': ((2,3),(1,1))#,(1, 2),(1,3)),  # Unigrams, Bigrams or Trigrams
}

#Classificaion
parameters={**feature_parameters,**clf_parameters} 
grid = GridSearchCV(pipeline,parameters,scoring='f1_micro',cv=10)          
grid.fit(train_x,train_y)     
clf= grid.best_estimator_  
print('********* Best Set of Parameters ********* \n\n')
print(clf)

predicted = clf.predict(test_x)
predicted =list(predicted)
df0 = evaluation(train_x,train_y,test_y,test_x,predicted,'random_forest',df0)

df0.to_csv('/content/bagofwords_on_models_result_csv.csv')

#

"""# word_to_vec

#logistic regression
"""

df_train = pd.DataFrame(data_train)
print('words_per before',df_train[0].apply(lambda x: len(x.split(' '))).sum())

df_train[0] = df_train[0].apply(clean_text, args= (4,))
print('words_per after',df_train[0].apply(lambda x: len(x.split(' '))).sum())
  
train_x, test_x, train_y, test_y = train_test_split(df_train, labels_train, test_size=0.2, random_state = 42)

test_tokenized = test_x.apply(lambda r: w2v_tokenize_text(r[0]), axis=1).values
train_tokenized = train_x.apply(lambda r: w2v_tokenize_text(r[0]), axis=1).values


X_train_word_average = word_averaging_list(wv,train_tokenized)
X_test_word_average = word_averaging_list(wv,test_tokenized)

# clf=RandomForestClassifier(random_state=42,n_estimators = 10)

# clf_parameters = {'criterion':('gini', 'entropy'),
#                   'max_depth' : (65,66),#,67,68,69,70),
#                   'max_features':('auto',0.75),#,0.8,0.4),
#                   }    
        
clf=LogisticRegression(class_weight='balanced') 
clf_parameters = {'solver':('newton-cg','lbfgs','liblinear'),}
 
grid = GridSearchCV(clf, clf_parameters, scoring='f1_micro',cv=10)          
grid.fit(X_train_word_average,train_y)     
clf= grid.best_estimator_  
print('********* Best Set of Parameters ********* \n\n')
print(clf)

predicted = clf.predict(X_test_word_average)
predicted =list(predicted)
df0 = evaluation(X_train_word_average,train_y,test_y,X_test_word_average,predicted,'logistic_regression',df0)

df_train = pd.DataFrame(data_train)
print('words_per before',df_train[0].apply(lambda x: len(x.split(' '))).sum())

df_train[0] = df_train[0].apply(clean_text, args= (4,))
print('words_per after',df_train[0].apply(lambda x: len(x.split(' '))).sum())
  
train_x, test_x, train_y, test_y = train_test_split(df_train, labels_train, test_size=0.2, random_state = 42)

test_tokenized = test_x.apply(lambda r: w2v_tokenize_text(r[0]), axis=1).values
train_tokenized = train_x.apply(lambda r: w2v_tokenize_text(r[0]), axis=1).values


X_train_word_average = word_averaging_list(wv,train_tokenized)
X_test_word_average = word_averaging_list(wv,test_tokenized)

clf=RandomForestClassifier(random_state=42,n_estimators = 10)

clf_parameters = {'criterion':('gini', 'entropy'),
                  'max_depth' : (65,66),#,67,68,69,70),
                  'max_features':('auto',0.75),#,0.8,0.4),
                  }    
        
# clf=LogisticRegression(class_weight='balanced') 
# clf_parameters = {'solver':('newton-cg','lbfgs','liblinear'),}
# clf=MultinomialNB(fit_prior=True, class_prior=None)

# clf_parameters = {'alpha':(0,1,2),}    
        
 
grid = GridSearchCV(clf, clf_parameters, scoring='f1_micro',cv=10)          
grid.fit(X_train_word_average,train_y)     
clf= grid.best_estimator_  
print('********* Best Set of Parameters ********* \n\n')
print(clf)

predicted = clf.predict(X_test_word_average)
predicted =list(predicted)
df0 = evaluation(X_train_word_average,train_y,test_y,X_test_word_average,predicted,'SVM',df0)

df_train = pd.DataFrame(data_train)
print('words_per before',df_train[0].apply(lambda x: len(x.split(' '))).sum())

df_train[0] = df_train[0].apply(clean_text, args= (4,))
print('words_per after',df_train[0].apply(lambda x: len(x.split(' '))).sum())
  
train_x, test_x, train_y, test_y = train_test_split(df_train, labels_train, test_size=0.2, random_state = 42)

test_tokenized = test_x.apply(lambda r: w2v_tokenize_text(r[0]), axis=1).values
train_tokenized = train_x.apply(lambda r: w2v_tokenize_text(r[0]), axis=1).values


X_train_word_average = word_averaging_list(wv,train_tokenized)
X_test_word_average = word_averaging_list(wv,test_tokenized)

# clf=RandomForestClassifier(random_state=42,n_estimators = 10)

# clf_parameters = {'criterion':('gini', 'entropy'),
#                   'max_depth' : (65,66),#,67,68,69,70),
#                   'max_features':('auto',0.75),#,0.8,0.4),
#                   }    
        
# clf=LogisticRegression(class_weight='balanced') 
# clf_parameters = {'solver':('newton-cg','lbfgs','liblinear'),}
clf=RandomForestClassifier(random_state=42,n_estimators = 10)

clf_parameters = {'criterion':('gini', 'entropy'),
                  'max_depth' : (65,66,67,68,69,70),
                  'max_features':('auto',0.75,0.8,0.4),
                  }  
 
grid = GridSearchCV(clf, clf_parameters, scoring='f1_micro',cv=10)          
grid.fit(X_train_word_average,train_y)     
clf= grid.best_estimator_  
print('********* Best Set of Parameters ********* \n\n')
print(clf)

predicted = clf.predict(X_test_word_average)
predicted =list(predicted)
df0 = evaluation(X_train_word_average,train_y,test_y,X_test_word_average,predicted,'random_forest',df0)

ss= df0.drop(0)
ss= ss.drop(1)

ss.to_csv('\content\wor2vec on classifier.csv')

ss= df1.drop(0)

ss